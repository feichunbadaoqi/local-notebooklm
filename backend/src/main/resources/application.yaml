spring:
  application:
    name: notebooklm

  # ============ SQLite Database ============
  datasource:
    # Enable WAL mode for better concurrency (multiple readers, single writer)
    url: jdbc:sqlite:data/notebooklm.db?journal_mode=WAL&busy_timeout=30000
    driver-class-name: org.sqlite.JDBC
    hikari:
      maximum-pool-size: 3  # WAL mode allows concurrent reads
      minimum-idle: 1
      connection-timeout: 30000
      idle-timeout: 60000

  jpa:
    database-platform: org.hibernate.community.dialect.SQLiteDialect
    hibernate:
      ddl-auto: update
    show-sql: false
    open-in-view: false  # Disable to avoid lazy loading issues in views
    properties:
      hibernate:
        format_sql: true

  # ============ File Upload ============
  servlet:
    multipart:
      max-file-size: 50MB
      max-request-size: 50MB

  # ============ Docker Compose ============
  docker:
    compose:
      lifecycle-management: start_only
      file: compose.yaml

# ============ Server Configuration ============
server:
  port: 8080

# ============ Elasticsearch ============
elasticsearch:
  host: localhost
  port: 9200
  scheme: http
  index:
    chunks: document_chunks

# ============ Application-Specific Elasticsearch Settings ============
app:
  elasticsearch:
    index-name: notebooklm-chunks
    chat-message-index-name: notebooklm-chat-messages
    memory-index-name: notebooklm-memories
    vector-dimensions: 3072
    # Text analyzer for multilingual support
    # Options:
    #   - standard: English only (default)
    #   - ik_max_word: Advanced Chinese indexing - fine-grained segmentation (requires IK Analysis plugin)
    #   - ik_smart: Advanced Chinese search - coarser, better for queries (requires IK Analysis plugin)
    # Docker: IK Analysis is auto-installed via Dockerfile.elasticsearch
    text-analyzer: ik_max_word
    text-search-analyzer: ik_smart

# ============ LangChain4j / OpenAI ============
langchain4j:
  openai:
    api-key: ${OPENAI_API_KEY:}
    chat-model:
      model-name: gpt-5-mini  # Fast, cost-efficient with vision support (400K context, $0.25/M input)
      max-completion-tokens: 4096
      timeout: 120s
    embedding-model:
      model-name: text-embedding-3-large  # Best embedding model
      dimensions: 3072

# ============ RAG Configuration ============
rag:
  chunking:
    size: 400  # Reduced to stay well under 8192 token embedding limit
    overlap: 50
  retrieval:
    top-k: 6
    rrf-k: 60  # Reciprocal Rank Fusion constant
    candidates-multiplier: 3  # Fetch 3x candidates for hybrid search
    source-anchoring-enabled: true   # Boost chunks from previous response docs on follow-ups
    source-anchoring-boost: 0.3      # Additive RRF boost for anchor document chunks
  reranking:
    strategy: tei  # "tei" (default, cross-encoder via TEI) or "llm" (OpenAI prompt-based)
    tei:
      base-url: http://localhost:8090
      model-id: BAAI/bge-reranker-base
      truncate: true
      raw-scores: false
      connect-timeout-ms: 5000
      read-timeout-ms: 10000
    llm:
      enabled: true
      batch-size: 30  # Fits EXPLORING mode: 8 topK Ã— 3 multiplier = 24 candidates in one batch
  compaction:
    sliding-window-size: 10
    token-threshold: 3000
    message-threshold: 30
    batch-size: 20
  memory:
    enabled: true
    max-per-session: 50        # Max memories per session
    extraction-threshold: 0.3  # Min importance to save
    context-limit: 5           # Max memories in context
    semantic-weight: 0.7       # Weight for semantic relevance (70%) vs importance (30%)
    candidate-pool-multiplier: 3  # Fetch 3x candidates for hybrid search
  query-reformulation:
    enabled: true              # Enable query reformulation for conversational RAG
    history-window: 5          # Number of relevant messages to return
    candidate-pool-multiplier: 4  # Fetch 4x more candidates for hybrid search
    max-query-length: 500      # Maximum length of reformulated queries
    min-recent-messages: 2     # Always fetch these from DB for recency-biased context
  verification:
    enabled: true              # Enable answer verification (Stage 4)
    support-threshold: 0.7     # Minimum score (0.0-1.0) for claim to be considered supported
  image-storage:
    base-path: data/images         # Root directory for images extracted from documents
    enabled: true                  # Set to false to skip image extraction
    max-file-size-bytes: 10485760  # 10 MB per image limit
  image-grouping:
    strategy: spatial              # Options: "spatial" (default) or "page-based"
    spatial:
      threshold: 100.0             # Distance threshold in PDF units (~1.4 inches at 72 DPI)
      min-group-size: 2            # Minimum images to form a group
    page-based:
      min-group-size: 2            # Minimum images on a page to form a group

# ============ Interaction Modes ============
modes:
  exploring:
    retrieval-count: 8
    description: "Broad discovery, tangential suggestions"
  research:
    retrieval-count: 4
    description: "Precise citations, fact-focused"
  learning:
    retrieval-count: 6
    description: "Socratic method, builds understanding"

# ============ Resilience4j ============
resilience4j:
  circuitbreaker:
    configs:
      default:
        register-health-indicator: true
        sliding-window-type: COUNT_BASED
        sliding-window-size: 10
        failure-rate-threshold: 50
        wait-duration-in-open-state: 30s
        permitted-number-of-calls-in-half-open-state: 3
        automatic-transition-from-open-to-half-open-enabled: true
    instances:
      openai:
        base-config: default
        failure-rate-threshold: 30
        wait-duration-in-open-state: 60s
      elasticsearch:
        base-config: default
        failure-rate-threshold: 50
        wait-duration-in-open-state: 10s
      tei:
        base-config: default
        failure-rate-threshold: 50
        wait-duration-in-open-state: 10s

  retry:
    configs:
      default:
        max-attempts: 3
        wait-duration: 1s
        enable-exponential-backoff: true
        exponential-backoff-multiplier: 2
    instances:
      openai:
        base-config: default
        max-attempts: 3
        wait-duration: 2s
        retry-exceptions:
          - java.net.SocketTimeoutException
          - java.io.IOException
      elasticsearch:
        base-config: default
        max-attempts: 3
        wait-duration: 500ms
      tei:
        base-config: default
        max-attempts: 2
        wait-duration: 500ms
        retry-exceptions:
          - java.net.SocketTimeoutException
          - java.io.IOException

  timelimiter:
    configs:
      default:
        timeout-duration: 30s
    instances:
      openai:
        timeout-duration: 60s
      elasticsearch:
        timeout-duration: 10s
      tei:
        timeout-duration: 15s

# ============ Actuator & Metrics ============
management:
  endpoints:
    web:
      exposure:
        include: health,info,metrics,prometheus,circuitbreakers,retries
  endpoint:
    health:
      show-details: always
      show-components: always
  metrics:
    tags:
      application: ${spring.application.name}
    distribution:
      percentiles-histogram:
        http.server.requests: true
      percentiles:
        http.server.requests: 0.5, 0.90, 0.95, 0.99
    export:
      prometheus:
        enabled: true

# ============ Logging ============
logging:
  level:
    root: INFO
    com.flamingo.ai.notebooklm: DEBUG
    org.hibernate.SQL: WARN
    org.springframework.web: INFO
    io.github.resilience4j: INFO
  pattern:
    console: "%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n"
